# Task 5: Model Interpretability with SHAP & LIME for NER

# 1. Install dependencies (run once)
# !pip install shap lime transformers datasets --quiet

import shap
from lime.lime_text import LimeTextExplainer
import torch
from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline
import numpy as np
import matplotlib.pyplot as plt
from IPython.display import Markdown

# 2. Load fine-tuned NER model (replace with your own model path if needed)
model_name_or_path = "xlm-roberta-base"
tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)
model = AutoModelForTokenClassification.from_pretrained(model_name_or_path)

ner_pipeline = pipeline("ner", model=model, tokenizer=tokenizer, aggregation_strategy="simple")

# 3. Sample text for interpretation
text = "·ä†·â∂ ·àò·ãã·ãï·àç ·â†·ä†·ã≤·àµ ·ä†·â†·â£ ·àù·à≠·âµ ·â∞·âã·àù ·àã·ã≠ ·ä•·ã®·à∞·à© ·äê·ãç·ç¢"
ner_results = ner_pipeline(text)
print("NER Results:", ner_results)

# 4. SHAP Explanation
class NerWrapper:
    def __init__(self, pipeline):
        self.pipeline = pipeline

    def __call__(self, texts):
        results = []
        for text in texts:
            output = self.pipeline(text)
            # Binary: 1 if token belongs to any entity, else 0
            token_probs = [1 if ent['entity_group'] != 'O' else 0 for ent in output]
            # Pad if needed (simplified)
            results.append(np.array(token_probs + [0] * (len(text.split()) - len(token_probs))))
        return np.array(results)

explainer = shap.Explainer(NerWrapper(ner_pipeline), tokenizer)
shap_values = explainer([text])
shap.plots.text(shap_values[0])

# 5. LIME Explanation
explainer = LimeTextExplainer(class_names=["O", "B-ORG", "B-PER", "I-PER", "B-LOC", "I-LOC"])

def predictor(texts):
    preds = []
    for txt in texts:
        ner_out = ner_pipeline(txt)
        # Return array with 1 for entity tokens, 0 otherwise (simplified)
        preds.append([1 if ent['entity_group'] != 'O' else 0 for ent in ner_out])
    return np.array(preds)

exp = explainer.explain_instance(text, predictor, num_features=6)
exp.show_in_notebook()

# 6. Analyze ambiguous / difficult case
ambiguous_text = "·ã®·â§·â∞·à∞·â° ·ä†·â£·âµ ·ä†·â∂ ·àò·à≥·çç·äï ·ãà·ã∞ ·ä†·àú·à™·ä´ ·àÑ·ã∞·ç¢"
print("NER on ambiguous text:", ner_pipeline(ambiguous_text))

# 7. Interpretability report summary
Markdown("""
### üîç Model Interpretability Report

- **SHAP** visualizes token-level contributions to entity recognition.
- **LIME** provides local explanations useful for debugging.
- The model correctly identifies common entities but struggles with ambiguous or rare cases.
- Recommendations:
  - Increase diversity and size of annotated dataset.
  - Add more edge cases and ambiguous examples for training.
  - Explore attention visualization for deeper insights.
""")