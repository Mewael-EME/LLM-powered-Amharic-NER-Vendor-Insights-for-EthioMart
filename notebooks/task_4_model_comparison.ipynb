# Task 4: Model Comparison & Selection - Interactive Notebook

# 1. Install / Import necessary libraries
!pip install transformers datasets sklearn --quiet

from datasets import Dataset
from transformers import (
    AutoTokenizer, AutoModelForTokenClassification,
    Trainer, TrainingArguments,
)
from sklearn.metrics import classification_report
import torch
import numpy as np

# Import your custom utilities (make sure your notebook kernel is set at repo root)
from src.data_loader import load_conll_data
from src.train_utils import align_labels_with_tokens  # or your label alignment function

# 2. Define labels and label mappings
LABELS = ['O', 'B-PER', 'I-PER', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG']  # example, adapt as needed
label2id = {label: i for i, label in enumerate(LABELS)}
id2label = {i: label for label, i in label2id.items()}

# 3. Load your dataset
DATA_PATH = "data/labeled/labeled_data.conll"
sentences, labels = load_conll_data(DATA_PATH)

# 4. Split data into train and validation sets
split_idx = int(0.8 * len(sentences))
train_sentences, val_sentences = sentences[:split_idx], sentences[split_idx:]
train_labels, val_labels = labels[:split_idx], labels[split_idx:]

# 5. Prepare tokenizer and dataset helper function
def tokenize_and_align_labels(tokenizer, sentences, labels):
    tokenized_inputs = tokenizer(
        sentences,
        is_split_into_words=True,
        padding=True,
        truncation=True,
        return_tensors="pt"
    )
    aligned_labels = []
    for i, label in enumerate(labels):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)
            elif word_idx != previous_word_idx:
                label_ids.append(label2id[label[word_idx]])
            else:
                current_label = label[word_idx]
                if current_label.startswith("B-"):
                    current_label = current_label.replace("B-", "I-")
                label_ids.append(label2id.get(current_label, label2id["O"]))
            previous_word_idx = word_idx
        aligned_labels.append(label_ids)
    return tokenized_inputs, aligned_labels

# 6. Define compute_metrics function for evaluation
def compute_metrics(pred):
    predictions, labels = pred
    predictions = np.argmax(predictions, axis=2)

    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]
    true_preds = [[id2label[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]

    report = classification_report(true_labels, true_preds, output_dict=True)
    f1_score = report['weighted avg']['f1-score']
    return {"f1": f1_score}

# 7. Models to try
models_to_try = [
    "xlm-roberta-base",
    "distilbert-base-uncased",
    "bert-base-multilingual-cased",
]

# 8. Fine-tune each model and evaluate
for model_name in models_to_try:
    print(f"\nTraining and evaluating {model_name} ...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForTokenClassification.from_pretrained(
        model_name,
        num_labels=len(LABELS),
        id2label=id2label,
        label2id=label2id
    )

    # Tokenize & align
    train_tokenized, train_aligned_labels = tokenize_and_align_labels(tokenizer, train_sentences, train_labels)
    val_tokenized, val_aligned_labels = tokenize_and_align_labels(tokenizer, val_sentences, val_labels)

    train_dataset = Dataset.from_dict({
        'input_ids': train_tokenized['input_ids'],
        'attention_mask': train_tokenized['attention_mask'],
        'labels': train_aligned_labels
    })
    val_dataset = Dataset.from_dict({
        'input_ids': val_tokenized['input_ids'],
        'attention_mask': val_tokenized['attention_mask'],
        'labels': val_aligned_labels
    })

    training_args = TrainingArguments(
        output_dir=f"./results/task4/{model_name.replace('/', '-')}",
        evaluation_strategy="epoch",
        save_strategy="epoch",
        learning_rate=2e-5,
        per_device_train_batch_size=8,
        per_device_eval_batch_size=8,
        num_train_epochs=3,
        weight_decay=0.01,
        logging_dir='./logs',
        logging_steps=10,
        load_best_model_at_end=True,
        metric_for_best_model="f1",
        greater_is_better=True,
        save_total_limit=1,
        seed=42,
        push_to_hub=False,
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
    )

    trainer.train()
    eval_results = trainer.evaluate()
    print(f"Evaluation results for {model_name}: {eval_results}")

# visualization or summarize the F1 scores here 
import matplotlib.pyplot as plt

# Suppose you collected results in a dictionary while training:
model_f1_scores = {}

for model_name in models_to_try:
    # After evaluation, store the F1 score (example placeholder, replace with actual result)
    # You can store during the loop above by appending results to this dictionary
    model_f1_scores[model_name] = 0.0  # Replace with actual F1 scores

# For demonstration, let's say you collected F1 scores like this (replace these with real values):
model_f1_scores = {
    "xlm-roberta-base": 0.85,
    "distilbert-base-uncased": 0.80,
    "bert-base-multilingual-cased": 0.82,
}

# Plot the comparison
plt.figure(figsize=(8,5))
plt.bar(model_f1_scores.keys(), model_f1_scores.values(), color='skyblue')
plt.title("Model F1 Score Comparison")
plt.ylabel("F1 Score")
plt.xlabel("Model")
plt.ylim(0,1)
plt.show()

# Print summary
best_model = max(model_f1_scores, key=model_f1_scores.get)
print(f"Best performing model: {best_model} with F1 score = {model_f1_scores[best_model]:.3f}")

